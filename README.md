# Knowledge-Distillation

Knowledge distillation is used to distill the  knowledge from a larger neural network( the teacher) to a smaller network (the student) so that it can be used on devices with limited hardware capabilities.The student model learns from teacher without any significant loss in accuracy.

Taking ideas from two research papers on knowledge distillation (Hinton,Fitnets), feature based knowledge distillation was implemented on the CIFAR-10 dataset.

